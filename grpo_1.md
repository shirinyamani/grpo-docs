# **Introduction to Group Relative Policy Optimization (GRPO)**  

## **What is GRPO?**  
Group Relative Policy Optimization (**GRPO**) is an **online reinforcement learning** algorithm designed to fine-tune large language models using **relative comparisons** of model-generated completions. Instead of training a separate value model (Critic), GRPO directly evaluates responses within groups, leading to **more efficient learning and lower computational costs**.

## **Why GRPO?**  
Traditional **Reinforcement Learning from Human Feedback (RLHF)** methods like **PPO (Proximal Policy Optimization)** rely on **training a value function** to estimate rewards. This can be **computationally expensive** and **prone to inaccuracies** if the value model generalizes poorly. GRPO eliminates this need by **comparing completions within groups**, making it more robust in domains where reward signals are clear and well-defined.

## **Core Components of GRPO**  
GRPO operates in **three main steps**:  

1. **Group Sampling**: The model generates multiple responses for a given prompt and evaluates them **relative to each other**.  
2. **Advantage Calculation**: Instead of using an external value model, GRPO calculates a **relative advantage** based on the rewards assigned to each response within the group.  
3. **Policy Update with Clipping & KL Regularization**: The policy is updated by **adjusting probabilities** in a controlled manner, using a **clipping mechanism** to stabilize training and **KL divergence** to prevent extreme shifts in behavior.


# Core intuition of GRPO
### **Goal**
By evaluating and comparing the completions generated by the policy model within groups, we eliminate the need to train a separate value model (Critic), thereby significantly reducing computational costs.

### **Application**: 
This approach is particularly effective in domains such as mathematical reasoning and code generation, where clear, rule-based reward structures are present, facilitating straightforward verification of outcomes.

# Steps of GRPO
## Step 1: Group Sampling

### Action  
For each question \( q \), the model generates \( G \) outputs (group size) using the previous policy model:

$$
(o_1, o_2, o_3, \dots, o_G) \sim \pi_{\theta_{\text{old}}}
$$

where \( G = 8 \), and each \( o_i \) represents a completion generated by the model.

### Example  

- **Question:**  
  - \( q \): Calculate \( 2 + 2 \times 6 \)  

- **Output:**  
  The model produces 8 responses (\( G = 8 \)):  

  $$
  o_1: 14 \, (\text{correct}), \quad o_2: 10 \, (\text{wrong}), \quad o_3: 16 \, (\text{wrong}), \quad \dots, \quad o_G: 14 \, (\text{correct})
  $$



## Step 2: **Advantage Calculation**

### **Reward Distribution**  
Each generated response is assigned an RM score \( r_i \) based on its correctness:

- \( r_i = 1 \) for a correct response
- \( r_i = 0 \) for an incorrect response  

Then, for each \( r_i \), we calculate its **Advantage Value**.

### **Advantage Value Formula**  
The advantage value \( A_i \) is computed as:

$$
A_i = \frac{r_i - \operatorname{mean}(\{r_1, r_2, \ldots, r_G\})}{\operatorname{std}(\{r_1, r_2, \ldots, r_G\})}
$$

where:
- \( \operatorname{mean}(\{r_1, r_2, \ldots, r_G\}) \) is the average reward of the group
- \( \operatorname{std}(\{r_1, r_2, \ldots, r_G\}) \) is the standard deviation of the rewards in the group

### **Example**  
Using the same example from **Step 1**, assume we have **8 responses**, where **4 are correct** and **4 are incorrect**:

- **Group Average:**  
  $$
  \operatorname{mean}(r_i) = 0.5
  $$  

- **Standard Deviation:**  
  $$
  \operatorname{std}(r_i) = 0.53
  $$  

- **Advantage Values:**
  - **Correct response:**  
    $$
    A_i = \frac{1 - 0.5}{0.53} = 0.94
    $$  
  - **Incorrect response:**  
    $$
    A_i = \frac{0 - 0.5}{0.53} = -0.94
    $$  

### **Interpretation**  
This **standardization process** (\( A_i \) scaling) helps the model assess the relative performance of each response, guiding the optimization process:

- If \( A_i > 0 \), the response \( o_i \) is **better than the group's average**, and its probability of being generated will **increase**.
- If \( A_i < 0 \), the response \( o_i \) is **worse than the group's average**, and its probability of being generated will **decrease**.

For example, since a **correct response** has \( A_i = 0.94 \), the optimization process will **increase** its generation probability, reinforcing good performance.

## Step 3: **Policy Update – Target Function**  

The target function for **Group Relative Policy Optimization (GRPO)** is defined as:

$$
J_{GRPO}(\theta) = \left[\frac{1}{G} \sum_{i=1}^{G} \min \left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} A_i, \text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1 - \epsilon, 1 + \epsilon \right) A_i \right)\right]- \beta D_{KL}(\pi_{\theta} || \pi_{ref})
$$

---

## **Key Components of the Target Function**  

### **1. Probability Ratio:**
$$
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}
$$  

This ratio measures how much the new model's response probability differs from the old model's probability, while incorporating a preference for responses that improve the expected outcome.

#### **Meaning:**
- If **ratio > 1**, the new model assigns a **higher probability** to response \( o_i \) compared to the old model.
- If **ratio < 1**, the new model assigns a **lower probability** to \( o_i \).

---

### **2. Clipping Function:**  
To **control drastic updates**, a clipping function is used:

$$
\text{clip}\left( \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1 - \epsilon, 1 + \epsilon \right)
$$  

This limits how much the probability ratio can change within the range **\[1 - \epsilon, 1 + \epsilon\]**, preventing excessive deviation from the old policy. The clipping mechanism stabilizes training by ensuring updates remain within a reasonable range.

---

### **Example – Suppose \( \epsilon = 0.2 \):**  

- **Case 1: Reinforcing a Correct Response**  
  Suppose the **new policy** assigns a probability of **0.9** to a specific response, while the **old policy** assigned **0.5**. This means the response is **reinforced**, but only within the controlled clipping limit:

  $$
  \text{Ratio} = \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} = \frac{0.9}{0.5} = 1.8  \quad \Rightarrow \quad \text{Clip} = 1.2 \quad (\text{upper bound limit})
  $$  

- **Case 2: Penalizing an Incorrect Response**  
  Suppose the **new policy** assigns a probability of **0.2**, while the **old policy** assigned **0.5**. This means the model **reduces** the probability of generating this response, but only within the controlled range:

  $$
  \text{Ratio} = \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} = \frac{0.2}{0.5} = 0.4  \quad \Rightarrow \quad \text{Clip} = 0.8 \quad (\text{lower bound limit})
  $$  

---

### **Interpretation**  
- The function encourages the new model to **increase the probability** of responses the old model underweighted, **if they improve the outcome**.
- If the old model **already favored a good response**, the new model can still reinforce it **but only within the range** \([1 - \epsilon, 1 + \epsilon]\).  
  - **Example:** If \( \epsilon = 0.2 \), then the probability ratio is restricted to **\[0.8, 1.2\]**.
- If the old model **overestimated a poor-quality response**, the new model is **discouraged** from maintaining that high probability.
- By incorporating the probability ratio, the objective function ensures that updates are **proportional to the advantage \( A_i \)** while preventing **instability due to excessive updates**.

## **3. KL Divergence:**  \( \beta D_{KL}(\pi_{\theta} || \pi_{ref}) \)

KL Divergence is used to **prevent over-optimization** of the reward model, which, in this context, refers to cases where the model outputs **nonsensical text** or, in a math reasoning scenario, generates **extremely incorrect answers**.

---

### **Example**
Suppose the reward model has a flaw, it **wrongly assigns higher rewards to incorrect outputs** due to spurious correlations in the training data.

- Consider the equation:
  
  $$
  2 + 2 \times 6 = 20
  $$  

- If the reward function mistakenly assigns **high confidence** to this incorrect response, it might lead to:

  $$
  R(o_6 = 20) = 0.95
  $$

- Without KL Divergence, during optimization, the model will **learn to favor responses with higher numerical values**, assuming they indicate *"more confident"* reasoning. Over time, the model might even start generating **completely incorrect but over-rewarded answers**, such as:

  $$
  2 + 2 \times 6 = 42
  $$  

- This response is no longer even an arithmetic mistake, it’s an instance of **reward hacking**. The model learns to **exploit** patterns that maximize the reward signal rather than correctly solving the problem.

---

### **Meaning**
- The KL divergence penalty **constrains** the model’s outputs, keeping them **closer to its original distribution** and **preventing extreme shifts**.
- Even if **incorrect answers receive high rewards**, the model **cannot deviate too much** from what it originally considered reasonable.
- Instead of **drifting towards irrational outputs**, the model refines its understanding while still allowing some exploration.
- KL divergence acts as a **regularization term**, preventing the model from blindly **optimizing for rewards** at the cost of logical correctness.

---

### **Mathematical Definition**
KL divergence is defined as:

$$
D_{KL}(P || Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}
$$

In **Reinforcement Learning from Human Feedback (RLHF)**, the two distributions of interest are:

- \( P(x) \) → **The new model’s policy**
- \( Q(x) \) → **The reference policy (e.g., the original model before fine-tuning)**

By minimizing KL divergence, the model maintains similarity to its reference while still optimizing for improved performance.

---

### **The Role of \( \beta \) in \( \beta D_{KL}(\pi_{\theta} || \pi_{ref}) \)**

- **Higher \( \beta \) (Stronger KL Penalty)**  
  - Puts **more constraint** on policy updates, keeping the model **closer to its reference distribution**.
  - Can **slow down adaptation**, making it harder for the model to explore better responses.
  
- **Lower \( \beta \) (Weaker KL Penalty)**  
  - Allows the model **more freedom** to update its policy and deviate from the reference.
  - Enables **faster adaptation**, but **risks instability** if not controlled.
  - **Over-optimization risk:** If the reward model is flawed, the policy might generate **nonsensical outputs**.

- **Recommended Value**  
  The **DeepSeekMath** ([DeepSeekMath Paper](https://arxiv.org/abs/2402.03300)) sets:

  $$
  \beta = 0.04
  $$

  as an optimal trade-off between **stability** and **exploration**.

---
# Complete Simple Math Example

## **Question**  
$$
\text{Q: Calculate} \quad 2 + 2 \times 6
$$

---

## **Step 1: Group Sampling**  
Generate \( G = 8 \) responses, where **4 are correct** (\( 14, \text{reward} = 1 \)) and **4 are incorrect** (\( \text{reward} = 0 \)).

Thus, the sampled outputs are:

$$
{o_1: 14 (\text{correct}), \quad o_2: 10 (\text{wrong}), \quad o_3: 16 (\text{wrong}), \quad \dots, \quad o_G: 14 (\text{correct})}
$$

---

## **Step 2: Advantage Calculation**  

- **Group Average Reward:**  
  $$
  \operatorname{mean}(r_i) = 0.5
  $$

- **Standard Deviation:**  
  $$
  \operatorname{std}(r_i) = 0.53
  $$

- **Advantage Values:**  
  - **Correct Response:**  
    $$
    A_i = \frac{1 - 0.5}{0.53} = 0.94
    $$  
  - **Incorrect Response:**  
    $$
    A_i = \frac{0 - 0.5}{0.53} = -0.94
    $$  

---

## **Step 3: Policy Update**  

- Assume the probability of the old policy \( \pi_{\theta_{old}} \) assigning a **correct response** \( o_1 \) was **0.5**.  
- The new policy \( \pi_{\theta} \) increases this probability to **0.7**.  

Thus, the probability ratio is:

$$
\text{Ratio} = \frac{0.7}{0.5} = 1.4
$$

Applying **clipping** with \( \epsilon = 0.2 \), we limit the ratio to:

$$
\text{Clipped Ratio} = 1.2
$$

- The target function **re-weights** the policy update, reinforcing the generation of **correct outputs**.
- **KL Divergence** prevents the new model from **drifting too far** from the reference policy.

---

## **Summary**
- The advantage function **increases** the probability of generating **correct responses** while **penalizing incorrect ones**.
- Clipping ensures **stable updates**, avoiding excessive jumps in probability.
- KL divergence **regularizes** learning, preventing over-optimization towards **flawed reward signals**.

